{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "N=8\n",
    "\n",
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, board_width, board_height):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(4*N*N,N*N)\n",
    "        \n",
    "        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(2*N*N, 64)\n",
    "        self.val_fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 4*N*N)\n",
    "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
    "        \n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 2*N*N)\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = F.tanh(self.val_fc2(x_val))\n",
    "        return x_act, x_val\n",
    "\n",
    "\n",
    "class PolicyValueNet():\n",
    "    def __init__(self , model_file=None, use_gpu=False):#输入：是否从已有model中载入，是否使用gpu\n",
    "        self.use_gpu = use_gpu\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.l2_const = 1e-4  # coef of l2 penalty\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.policy_value_net = Net(board_width, board_height).cuda()\n",
    "        else:\n",
    "            self.policy_value_net = Net(board_width, board_height)\n",
    "        self.optimizer = optim.Adam(self.policy_value_net.parameters(),weight_decay=self.l2_const)\n",
    "\n",
    "        if model_file:\n",
    "            self.policy_value_net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def policy_value(self, state_batch):#得到所有位置下棋的概率\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            act_probs = np.exp(log_act_probs.data.cpu().numpy())\n",
    "            return act_probs, value.data.cpu().numpy()\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            act_probs = np.exp(log_act_probs.data.numpy())\n",
    "            return act_probs, value.data.numpy()\n",
    "\n",
    "    def policy_value_fn(self, Board):#得到所有可行位置下棋的概率\n",
    "        legal_positions = board.get_availables()\n",
    "        current_state = np.ascontiguousarray(board.Board.reshape(-1, 1, N, N))\n",
    "        if self.use_gpu:\n",
    "            log_act_probs, value = self.policy_value_net(Variable(torch.from_numpy(current_state)).cuda().float())\n",
    "            act_probs = np.exp(log_act_probs.data.cpu().numpy().flatten())\n",
    "        else:\n",
    "            log_act_probs, value = self.policy_value_net(Variable(torch.from_numpy(current_state)).float())\n",
    "            act_probs = np.exp(log_act_probs.data.numpy().flatten())\n",
    "        act_probs = zip(legal_positions, act_probs[legal_positions])\n",
    "        value = value.data[0][0]\n",
    "        return act_probs, value\n",
    "\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
    "        #输入：棋盘，策略，该方是否胜利，lr \n",
    "        #输出：loss & policy entropy\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs).cuda())\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch).cuda())\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs))\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        set_learning_rate(self.optimizer, lr)\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        \n",
    "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
    "        policy_loss = -torch.mean(torch.sum(mcts_probs*log_act_probs, 1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # calc policy entropy, for monitoring only\n",
    "        entropy = -torch.mean(torch.sum(torch.exp(log_act_probs) * log_act_probs, 1))\n",
    "        return loss.data[0], entropy.data[0]\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        torch.save(self.policy_value_net.state_dict(), model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
